{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a baseline model\n",
    "\n",
    "In this notebook we create a baseline solution to our semantic segmentation problem. To iterate fast we use a notebook here. We will then refactor this code into a script to be able to use hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.wandb import WandbCallback\n",
    "\n",
    "import params\n",
    "# Helper functions - for example metrics we will track during our experiments\n",
    "from utils import get_predictions, create_iou_table, MIOU, BackgroundIOU, \\\n",
    "                  RoadIOU, TrafficLightIOU, TrafficSignIOU, PersonIOU, VehicleIOU, BicycleIOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "Create a train_config that gets passed to the W&B `run` to control training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleNamespace - creates an object to store values as attributes without creating your own (almost empty) class.\n",
    "\n",
    "train_config = SimpleNamespace(\n",
    "    framework=\"fastai\",\n",
    "    img_size=(180, 320),\n",
    "    batch_size=8,\n",
    "    augment=True, # use data augmentation\n",
    "    epochs=10, \n",
    "    lr=2e-3,\n",
    "    pretrained=True,  # whether to use pretrained encoder\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "set_seed(train_config.seed, reproducible=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33md-oliver-cort\u001b[0m (\u001b[33mdoc93\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidoc/Documents/personal/MLOps-model-development/1_build_end2end_prototype/wandb/run-20230804_161959-5a83ruvy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doc93/mlops-course-001/runs/5a83ruvy' target=\"_blank\">genial-silence-5</a></strong> to <a href='https://wandb.ai/doc93/mlops-course-001' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doc93/mlops-course-001' target=\"_blank\">https://wandb.ai/doc93/mlops-course-001</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doc93/mlops-course-001/runs/5a83ruvy' target=\"_blank\">https://wandb.ai/doc93/mlops-course-001/runs/5a83ruvy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inputs\n",
    "# - pass train_config into W&B run to control training hyperparameters\n",
    "# - project=params.WANDB_PROJECT to make this W&B run be part of same project as previous  notebook W&B runs\n",
    "run = wandb.init(project=params.WANDB_PROJECT, entity=params.ENTITY, job_type=\"training\", config=train_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use W&B `artifacts` to track the lineage of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact bdd_simple_1k_split:latest, 846.07MB. 4010 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   4010 of 4010 files downloaded.  \n",
      "Done. 0:0:11.5\n"
     ]
    }
   ],
   "source": [
    "# Use artefacts to track the data linage of our models\n",
    "processed_data_artifact = run.use_artifact(f'{params.PROCESSED_DATA_AT}:latest')\n",
    "\n",
    "# Download split data from W&B artifact\n",
    "processed_dataset_dir = Path(processed_data_artifact.download())\n",
    "\n",
    "# Read csv containing data split data (train/valid/test)\n",
    "df = pd.read_csv(processed_dataset_dir / 'data_split.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove test set rows\n",
    "df = df[df.Stage != 'test'].reset_index(drop=True)\n",
    "# - is_valid column will tell our trainer how we want to split data between training and validation.\n",
    "df['is_valid'] = df.Stage == 'valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(fname):\n",
    "    return (fname.parent.parent/\"labels\")/f\"{fname.stem}_mask.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add image and mask label paths to dataframe\n",
    "df[\"image_fname\"] = [processed_dataset_dir/f'images/{f}' for f in df.File_Name.values]\n",
    "df[\"label_fname\"] = [label_func(f) for f in df.image_fname.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "We use fastai's DataBlock API to feed data into model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df, bs=4, img_size=(180, 320), augment=True):\n",
    "    block = DataBlock(blocks=(ImageBlock, MaskBlock(codes=params.BDD_CLASSES)),\n",
    "                  get_x=ColReader(\"image_fname\"),\n",
    "                  get_y=ColReader(\"label_fname\"),\n",
    "                  splitter=ColSplitter(),\n",
    "                  item_tfms=Resize(img_size),\n",
    "                  batch_tfms=aug_transforms() if augment else None,\n",
    "                 )\n",
    "    return block.dataloaders(df, bs=bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `wandb.config` to track our training hyperparameters (config parameters defined in `wandb.init(... , config=config_file)` )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not do one pass in your dataloader, there is something wrong in it. Please see the stack trace below:\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::_linalg_solve_ex.result' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dls \u001b[39m=\u001b[39m get_data(df, bs\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbatch_size, img_size\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mimg_size, augment\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49maugment)\n",
      "Cell \u001b[0;32mIn[9], line 9\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(df, bs, img_size, augment)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(df, bs\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, img_size\u001b[39m=\u001b[39m(\u001b[39m180\u001b[39m, \u001b[39m320\u001b[39m), augment\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     block \u001b[39m=\u001b[39m DataBlock(blocks\u001b[39m=\u001b[39m(ImageBlock, MaskBlock(codes\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mBDD_CLASSES)),\n\u001b[1;32m      3\u001b[0m                   get_x\u001b[39m=\u001b[39mColReader(\u001b[39m\"\u001b[39m\u001b[39mimage_fname\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m                   get_y\u001b[39m=\u001b[39mColReader(\u001b[39m\"\u001b[39m\u001b[39mlabel_fname\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                   batch_tfms\u001b[39m=\u001b[39maug_transforms() \u001b[39mif\u001b[39;00m augment \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                  )\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mreturn\u001b[39;00m block\u001b[39m.\u001b[39;49mdataloaders(df, bs\u001b[39m=\u001b[39;49mbs)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/data/block.py:157\u001b[0m, in \u001b[0;36mDataBlock.dataloaders\u001b[0;34m(self, source, path, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m dsets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatasets(source, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m    156\u001b[0m kwargs \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs, \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m: verbose}\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m dsets\u001b[39m.\u001b[39;49mdataloaders(path\u001b[39m=\u001b[39;49mpath, after_item\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitem_tfms, after_batch\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_tfms, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/data/core.py:337\u001b[0m, in \u001b[0;36mFilteredBase.dataloaders\u001b[0;34m(self, bs, shuffle_train, shuffle, val_shuffle, n, path, dl_type, dl_kwargs, device, drop_last, val_bs, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m dl \u001b[39m=\u001b[39m dl_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset(\u001b[39m0\u001b[39m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(kwargs,def_kwargs, dl_kwargs[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    336\u001b[0m def_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mbs\u001b[39m\u001b[39m'\u001b[39m:bs \u001b[39mif\u001b[39;00m val_bs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m val_bs,\u001b[39m'\u001b[39m\u001b[39mshuffle\u001b[39m\u001b[39m'\u001b[39m:val_shuffle,\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mNone\u001b[39;00m,\u001b[39m'\u001b[39m\u001b[39mdrop_last\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m}\n\u001b[0;32m--> 337\u001b[0m dls \u001b[39m=\u001b[39m [dl] \u001b[39m+\u001b[39m [dl\u001b[39m.\u001b[39mnew(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset(i), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n\u001b[1;32m    338\u001b[0m               \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_subsets)]\n\u001b[1;32m    339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbunch_type(\u001b[39m*\u001b[39mdls, path\u001b[39m=\u001b[39mpath, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/data/core.py:337\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    335\u001b[0m dl \u001b[39m=\u001b[39m dl_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubset(\u001b[39m0\u001b[39m), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerge(kwargs,def_kwargs, dl_kwargs[\u001b[39m0\u001b[39m]))\n\u001b[1;32m    336\u001b[0m def_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mbs\u001b[39m\u001b[39m'\u001b[39m:bs \u001b[39mif\u001b[39;00m val_bs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m val_bs,\u001b[39m'\u001b[39m\u001b[39mshuffle\u001b[39m\u001b[39m'\u001b[39m:val_shuffle,\u001b[39m'\u001b[39m\u001b[39mn\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mNone\u001b[39;00m,\u001b[39m'\u001b[39m\u001b[39mdrop_last\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mFalse\u001b[39;00m}\n\u001b[0;32m--> 337\u001b[0m dls \u001b[39m=\u001b[39m [dl] \u001b[39m+\u001b[39m [dl\u001b[39m.\u001b[39;49mnew(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubset(i), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmerge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n\u001b[1;32m    338\u001b[0m               \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_subsets)]\n\u001b[1;32m    339\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbunch_type(\u001b[39m*\u001b[39mdls, path\u001b[39m=\u001b[39mpath, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/data/core.py:97\u001b[0m, in \u001b[0;36mTfmdDL.new\u001b[0;34m(self, dataset, cls, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_n_inp\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_types\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     96\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_one_pass()\n\u001b[1;32m     98\u001b[0m         res\u001b[39m.\u001b[39m_n_inp,res\u001b[39m.\u001b[39m_types \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_inp,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_types\n\u001b[1;32m     99\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e: \n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/data/core.py:80\u001b[0m, in \u001b[0;36mTfmdDL._one_pass\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_batch([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_item(\u001b[39mNone\u001b[39;00m)])\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: b \u001b[39m=\u001b[39m to_device(b, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 80\u001b[0m its \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mafter_batch(b)\n\u001b[1;32m     81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_inp \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(its, (\u001b[39mlist\u001b[39m,\u001b[39mtuple\u001b[39m)) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(its)\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(its)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_types \u001b[39m=\u001b[39m explode_types(its)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:208\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m compose_tfms(o, tfms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs, split_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_idx)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[0;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tfms:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_enc: f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mdecode\n\u001b[0;32m--> 158\u001b[0m     x \u001b[39m=\u001b[39m f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:49\u001b[0m, in \u001b[0;36mRandTransform.__call__\u001b[0;34m(self, b, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m     44\u001b[0m     b, \n\u001b[1;32m     45\u001b[0m     split_idx:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m# Index of the train/valid dataset\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     47\u001b[0m ):\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_call(b, split_idx\u001b[39m=\u001b[39msplit_idx)\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(b, split_idx\u001b[39m=\u001b[39;49msplit_idx, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo \u001b[39melse\u001b[39;00m b\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m---> 81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m'\u001b[39;49m\u001b[39mencodes\u001b[39;49m\u001b[39m'\u001b[39;49m, x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, fn, x, split_idx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m split_idx\u001b[39m!=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, fn), x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(f, x_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;49;00m x_ \u001b[39min\u001b[39;49;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(f, x_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), x, ret)\n\u001b[1;32m     98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(f, x_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:501\u001b[0m, in \u001b[0;36mAffineCoordTfm.encodes\u001b[0;34m(self, x)\u001b[0m\n\u001b[0;32m--> 501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencodes\u001b[39m(\u001b[39mself\u001b[39m, x:TensorImage): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:499\u001b[0m, in \u001b[0;36mAffineCoordTfm._encode\u001b[0;34m(self, x, mode, reverse)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode\u001b[39m(\u001b[39mself\u001b[39m, x, mode, reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     coord_func \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoord_fs)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39melse\u001b[39;00m partial(compose_tfms, tfms\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoord_fs, reverse\u001b[39m=\u001b[39mreverse)\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49maffine_coord(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmat, coord_func, sz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, mode\u001b[39m=\u001b[39;49mmode, pad_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_mode, align_corners\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malign_corners)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:390\u001b[0m, in \u001b[0;36maffine_coord\u001b[0;34m(x, mat, coord_tfm, sz, mode, pad_mode, align_corners)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[39mif\u001b[39;00m mat \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: mat \u001b[39m=\u001b[39m _init_mat(x)[:,:\u001b[39m2\u001b[39m]\n\u001b[1;32m    389\u001b[0m coords \u001b[39m=\u001b[39m affine_grid(mat, x\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m size, align_corners\u001b[39m=\u001b[39malign_corners)\n\u001b[0;32m--> 390\u001b[0m \u001b[39mif\u001b[39;00m coord_tfm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: coords \u001b[39m=\u001b[39m coord_tfm(coords)\n\u001b[1;32m    391\u001b[0m \u001b[39mreturn\u001b[39;00m TensorImage(_grid_sample(x, coords, mode\u001b[39m=\u001b[39mmode, padding_mode\u001b[39m=\u001b[39mpad_mode, align_corners\u001b[39m=\u001b[39malign_corners))\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastcore/transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[0;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tfms:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_enc: f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mdecode\n\u001b[0;32m--> 158\u001b[0m     x \u001b[39m=\u001b[39m f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:875\u001b[0m, in \u001b[0;36m_WarpCoord.__call__\u001b[0;34m(self, x, invert)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, invert\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 875\u001b[0m     coeffs \u001b[39m=\u001b[39m find_coeffs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarg_pts, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_pts) \u001b[39mif\u001b[39;00m invert \u001b[39melse\u001b[39;00m find_coeffs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_pts, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarg_pts)\n\u001b[1;32m    876\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_perspective(x, coeffs)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:839\u001b[0m, in \u001b[0;36mfind_coeffs\u001b[0;34m(p1, p2)\u001b[0m\n\u001b[1;32m    837\u001b[0m A \u001b[39m=\u001b[39m stack(m)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    838\u001b[0m B \u001b[39m=\u001b[39m p1\u001b[39m.\u001b[39mview(p1\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m8\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 839\u001b[0m \u001b[39mreturn\u001b[39;00m solve(A,B)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/vision/augment.py:816\u001b[0m, in \u001b[0;36m_linalg_solve\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_linalg_solve\u001b[39m(A,B):\n\u001b[0;32m--> 816\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msolve(A,B)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/fastai/torch_core.py:382\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> 382\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    383\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    384\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/personal/MLOps-model-development/venv_wnb/lib/python3.8/site-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::_linalg_solve_ex.result' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "dls = get_data(df, bs=config.batch_size, img_size=config.img_size, augment=config.augment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m metrics \u001b[39m=\u001b[39m [MIOU(), BackgroundIOU(), RoadIOU(), TrafficLightIOU(), \\\n\u001b[1;32m      3\u001b[0m            TrafficSignIOU(), PersonIOU(), VehicleIOU(), BicycleIOU()]\n\u001b[1;32m      5\u001b[0m \u001b[39m# The model is a unet based on a pretrained resnet18 backbone.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m learn \u001b[39m=\u001b[39m unet_learner(dls, arch\u001b[39m=\u001b[39mresnet18, pretrained\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mpretrained, metrics\u001b[39m=\u001b[39mmetrics)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
     ]
    }
   ],
   "source": [
    "# We use intersection over union metrics: mean across all classes (MIOU) and IOU for each class separately.\n",
    "metrics = [MIOU(), BackgroundIOU(), RoadIOU(), TrafficLightIOU(), \\\n",
    "           TrafficSignIOU(), PersonIOU(), VehicleIOU(), BicycleIOU()]\n",
    "\n",
    "# The model is a unet based on a pretrained resnet18 backbone.\n",
    "learn = unet_learner(dls, arch=resnet18, pretrained=config.pretrained, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Fastai` already has a callback that integrates tightly with W&B. Only need to pass the `WandbCallback` to the `learner` (setup model) and we are ready to go. The callback will `log` all the useful variables for us. For example, whatever metric we pass to the learner will be tracked by the callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    SaveModelCallback(monitor='miou'),              # Save model with best miou metric\n",
    "    WandbCallback(log_preds=False, log_model=True)  # We log predictions manually on W&B (so set log_preds=False), and we want to log model W&B (so log_model=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "learn.fit_one_cycle(config.epochs, config.lr, cbs=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log a `table` with model predictions and ground truth, to W&B, so that we can do `error analysis` in the W&B dashboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, outputs, predictions = get_predictions(learn)\n",
    "table = create_iou_table(samples, outputs, predictions, params.BDD_CLASSES)\n",
    "wandb.log({\"pred_table\":table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the scores of the model from the best checkpoint. \n",
    "\n",
    "To make sure we track the final metrics correctly, we will validate the model again and save the final loss and metrics to wandb.summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = learn.validate()\n",
    "metric_names = ['final_loss'] + [f'final_{x.name}' for x in metrics]\n",
    "final_results = {metric_names[i] : scores[i] for i in range(len(scores))}\n",
    "for k,v in final_results.items(): \n",
    "    wandb.summary[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wnb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
